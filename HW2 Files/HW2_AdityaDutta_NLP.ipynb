{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988773b2",
   "metadata": {},
   "source": [
    "# 1. DATASET GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24870401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_11396\\2823314214.py:8: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, sep='\\t', header=0, on_bad_lines='skip', usecols=['star_rating', 'review_body'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset:\n",
      "         rating                                             review  sentiment\n",
      "1735859       2  I'm not sure the pre-owned pen is not a knock-...          2\n",
      "1924721       2  I ordered two cartridges and only one worked. ...          2\n",
      "1037272       4  Works well for shipping comic books but is con...          1\n",
      "710208        2  Does not work with www.stamps.com. Which I und...          2\n",
      "811349        3  It's good, but the adhesives need to be stickier.          3\n",
      "\n",
      "Testing Dataset:\n",
      "         rating                                             review  sentiment\n",
      "2054240       1  This order was for two color ink cartridges. W...          2\n",
      "1530392       2  I love Avery products but these sheet protecto...          2\n",
      "2590754       1  Imagine pressing a key (like Enter or the deci...          2\n",
      "1539774       3  I like the color but I can't keep it flowing i...          3\n",
      "1174910       4  So far I like it - it's fast and crisp and was...          1\n",
      "X_train shape: (200000,)\n",
      "Y_train shape: (200000,)\n",
      "X_test shape: (50000,)\n",
      "Y_test shape: (50000,)\n"
     ]
    }
   ],
   "source": [
    "#DATASET GENERATION\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#  file path\n",
    "file_path = r\"C:\\Users\\adity\\Desktop\\NLP\\amazon_reviews_us_Office_Products_v1_00.tsv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_path, sep='\\t', header=0, on_bad_lines='skip', usecols=['star_rating', 'review_body'])\n",
    "df.columns = ['rating', 'review']  # Rename columns for consistency\n",
    "\n",
    "df = df.dropna()  # Drop rows with missing values\n",
    "df['rating'] = df['rating'].astype(int)  \n",
    "\n",
    "# Balance dataset to have 50K instances per rating score\n",
    "frames = []  # List to hold data frames to concatenate\n",
    "for rating in range(1, 6):\n",
    "    subset = df[df['rating'] == rating]\n",
    "    if len(subset) >= 50000:\n",
    "        frames.append(subset.sample(n=50000, random_state=42))\n",
    "    else:\n",
    "        frames.append(subset)\n",
    "        \n",
    "# Concatenate all frames to form the balanced dataset\n",
    "balanced_df = pd.concat(frames)\n",
    "\n",
    "# ternary labels created\n",
    "def label_sentiment(row):\n",
    "    if row['rating'] > 3:\n",
    "        return 1  # Positive \n",
    "    elif row['rating'] < 3:\n",
    "        return 2  # Negative \n",
    "    else:\n",
    "        return 3  # Neutral \n",
    "\n",
    "#labeling function \n",
    "balanced_df['sentiment'] = balanced_df.apply(label_sentiment, axis=1)\n",
    "\n",
    "# Splitting the dataset into training and testing sets (80%/20%)\n",
    "train_df, test_df = train_test_split(balanced_df, test_size=0.2, random_state=42)\n",
    "\n",
    "#  target variable for training and testing data\n",
    "X_train = train_df['review']\n",
    "Y_train = train_df['sentiment']\n",
    "X_test = test_df['review']\n",
    "Y_test = test_df['sentiment']\n",
    "\n",
    "# save for later use\n",
    "train_df.to_csv('train_dataset.csv', index=False)\n",
    "test_df.to_csv('test_dataset.csv', index=False)\n",
    "\n",
    "print(\"Training Dataset:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTesting Dataset:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# Print shapes of X_train, Y_train, X_test, and Y_test\n",
    "# to confirm the size and structure of the data splits\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9111f7eb",
   "metadata": {},
   "source": [
    "# 2. WORD EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a655ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'king' and 'queen': 0.6510956883430481\n",
      "Similarity between 'excellent' and 'outstanding': 0.556748628616333\n",
      "King - Man + Woman = [('queen', 0.7118193507194519)]\n"
     ]
    }
   ],
   "source": [
    "#  2. WORD EMBEDDING\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# path to googlecWord2Vec binary file\n",
    "model_path = r\"C:\\Users\\adity\\Desktop\\NLP\\GoogleNews-vectors-negative300.bin.gz\"\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "# The model is loaded in a binary format to optimize memory usage\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "# Example words \n",
    "words = ['king', 'man', 'woman', 'queen', 'excellent', 'outstanding']\n",
    "\n",
    "# Check similarity between 'king' and 'queen' and between 'excellent' and 'outstanding'\n",
    "similarity_king_queen = model.similarity('king', 'queen')\n",
    "similarity_excellent_outstanding = model.similarity('excellent', 'outstanding')\n",
    "\n",
    "print(f\"Similarity between 'king' and 'queen': {similarity_king_queen}\")\n",
    "print(f\"Similarity between 'excellent' and 'outstanding': {similarity_excellent_outstanding}\")\n",
    "\n",
    "# Performing a vector algebra operation to find a word that best fits the relationship: King - Man + Woman.\n",
    "# Example: King - Man + Woman -> ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(\"King - Man + Woman =\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cdcab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'king' and 'queen' in my model: 0.45475128293037415\n",
      "Similarity between 'excellent' and 'outstanding' in my model: 0.8282253742218018\n",
      "King - Man + Woman in my model = [('magnum', 0.4322156310081482)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Preprocess the reviews from your balanced dataset.\n",
    "# The dataset 'balanced_df' was prepared in Part 1, ensuring an equal distribution of ratings.\n",
    "# Each review is converted to a list of tokens (words), with simple preprocessing applied to each.\n",
    "# 'balanced_df' is my dataset from Part 1\n",
    "reviews = balanced_df['review'].astype(str).tolist()\n",
    "tokenized_reviews = [simple_preprocess(review) for review in reviews]\n",
    "\n",
    "# 'vector_size=300' sets the size of the word vectors.\n",
    "# 'window=11' defines the maximum distance between the current and predicted word within a sentence.\n",
    "# 'min_count=10' ignores all words with total frequency lower than this.\n",
    "# 'workers=4' sets the number of worker threads to use for training.\n",
    "# TraiNING MY Word2Vec model\n",
    "my_model = Word2Vec(sentences=tokenized_reviews, vector_size=300, window=11, min_count=10, workers=4)\n",
    "\n",
    "# semantic similarities with examples \n",
    "try:\n",
    "    similarity_king_queen_my_model = my_model.wv.similarity('king', 'queen')\n",
    "    similarity_excellent_outstanding_my_model = my_model.wv.similarity('excellent', 'outstanding')\n",
    "    print(f\"Similarity between 'king' and 'queen' in my model: {similarity_king_queen_my_model}\")\n",
    "    print(f\"Similarity between 'excellent' and 'outstanding' in my model: {similarity_excellent_outstanding_my_model}\")\n",
    "except KeyError as e:\n",
    "    # This block catches the case where the words 'king', 'queen', 'excellent', or 'outstanding' are not in the vocabulary.\n",
    "    # This could happen if the words were not frequent enough in the dataset or were removed during preprocessing.\n",
    "    print(f\"Word not in vocabulary: {e}\")\n",
    "\n",
    "# Example: King - Man + Woman in MY model\n",
    "try:\n",
    "    result_my_model = my_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "    print(\"King - Man + Woman in my model =\", result_my_model)\n",
    "except KeyError as e:\n",
    "    print(f\"Word not in vocabulary for my model: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddbbda0",
   "metadata": {},
   "source": [
    "# 3.  Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd9a3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Loading pre-trained Google News Word2Vec model\n",
    "google_news_path = r\"C:\\Users\\adity\\Desktop\\NLP\\GoogleNews-vectors-negative300.bin.gz\"\n",
    "# The 'binary=True' parameter indicates that the model file is in a binary format.\n",
    "google_model = KeyedVectors.load_word2vec_format(google_news_path, binary=True)\n",
    "\n",
    "# using google_model to compute average Word2Vec vectors\n",
    "\n",
    "\n",
    "def average_word2vec(model, reviews, vector_size):\n",
    "    vectors = []\n",
    "    for review in reviews:\n",
    "        words = simple_preprocess(review)\n",
    "        word_vectors = [model[word] for word in words if word in model]\n",
    "        if word_vectors:\n",
    "            vectors.append(np.mean(word_vectors, axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(vector_size))\n",
    "    return np.array(vectors)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "778f4bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_perceptron(X_train, Y_train, X_test, Y_test):\n",
    "    perceptron = Perceptron()\n",
    "    # X_train contains the feature vectors for the training set,\n",
    "    # and Y_train contains the corresponding labels.\n",
    "    perceptron.fit(X_train, Y_train)\n",
    "     # X_test contains the feature vectors for the testing set.\n",
    "    Y_pred_perceptron = perceptron.predict(X_test)\n",
    "     # It compares the predicted labels (Y_pred_perceptron) against the actual labels (Y_test)\n",
    "    # and returns the proportion of correctly predicted labels.\n",
    "    accuracy = accuracy_score(Y_test, Y_pred_perceptron)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09a7fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(X_train, Y_train, X_test, Y_test):\n",
    "    svm = SVC()\n",
    "    svm.fit(X_train, Y_train)\n",
    "    Y_pred_svm = svm.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred_svm)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9955e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'google_model' is  pre-trained Word2Vec model loaded from word2vec-google-news-300\n",
    "X_train_google_w2v = average_word2vec(google_model, X_train, 300)\n",
    "X_test_google_w2v = average_word2vec(google_model, X_test, 300)\n",
    "\n",
    "accuracy_perceptron_google = train_perceptron(X_train_google_w2v, Y_train, X_test_google_w2v, Y_test)\n",
    "print(\"Perceptron Accuracy with Google News Word2Vec:\", accuracy_perceptron_google)\n",
    "\n",
    "accuracy_svm_google = train_svm(X_train_google_w2v, Y_train, X_test_google_w2v, Y_test)\n",
    "print(\"SVM Accuracy with Google News Word2Vec:\", accuracy_svm_google)\n",
    "\n",
    "\n",
    "\n",
    "# 'my_model' \n",
    "X_train_my_w2v = average_word2vec(my_model, X_train, 300)\n",
    "X_test_my_w2v = average_word2vec(my_model, X_test, 300)\n",
    "\n",
    "accuracy_perceptron_my = train_perceptron(X_train_my_w2v, Y_train, X_test_my_w2v, Y_test)\n",
    "print(\"Perceptron Accuracy with My Word2Vec:\", accuracy_perceptron_my)\n",
    "\n",
    "accuracy_svm_my = train_svm(X_train_my_w2v, Y_train, X_test_my_w2v, Y_test)\n",
    "print(\"SVM Accuracy with My Word2Vec:\", accuracy_svm_my)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07692fa",
   "metadata": {},
   "source": [
    "# 4. FEEDFORWARD NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d86ab0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\adity\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\adity\\anaconda3\\lib\\site-packages (0.17.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\adity\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# 4. FEEDFORWARD NEURAL NETWORKS \n",
    "\n",
    "!pip install torch torchvision\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15167589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aab7d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d800d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the average Word2Vec vectors\n",
    "def average_word2vec(model, reviews, vector_size):\n",
    "    vectors = []\n",
    "    for review in reviews:\n",
    "        words = simple_preprocess(review)\n",
    "        word_vectors = [model.wv[word] for word in words if word in model.wv.key_to_index]\n",
    "        if word_vectors:\n",
    "            vectors.append(np.mean(word_vectors, axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(vector_size))\n",
    "    return np.array(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bbd63c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['rating', 'review'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(balanced_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10893c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(balanced_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extracting the reviews and ratings\n",
    "X_train = train_df['review'].tolist()\n",
    "y_train = train_df['rating']\n",
    "X_test = test_df['review'].tolist()\n",
    "y_test = test_df['rating']\n",
    "\n",
    "train_embeddings = average_word2vec(my_model, X_train, vector_size=300)\n",
    "test_embeddings = average_word2vec(my_model, X_test, vector_size=300)\n",
    "\n",
    "y_train_zero_indexed = y_train - 1\n",
    "y_test_zero_indexed = y_test - 1\n",
    "\n",
    "# Converting the embeddings and labels into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(train_embeddings, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(test_embeddings, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Creating TensorDatasets and DataLoaders\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0cf06e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, train_loader, num_epochs):\n",
    "    model.train()  \n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()  \n",
    "            outputs = model(inputs) \n",
    "            loss = criterion(outputs, labels)  \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "        print(f'Epoch {epoch+1}/{num_epochs} finished')\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  \n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1) \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "893fad13",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 2 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Train and evaluate binary model\u001b[39;00m\n\u001b[0;32m     20\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# You can adjust this\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m train_model(binary_model, optimizer_binary, criterion, train_loader_binary, num_epochs)\n\u001b[0;32m     22\u001b[0m binary_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(binary_model, test_loader_binary)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBinary classification accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbinary_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[38], line 7\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, criterion, train_loader, num_epochs)\u001b[0m\n\u001b[0;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Reset gradients to zero for each batch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m      8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 2 is out of bounds."
     ]
    }
   ],
   "source": [
    "# Preprocess labels\n",
    "y_train = y_train - 1  \n",
    "y_test = y_test - 1\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 64\n",
    "train_loader_binary = DataLoader(TensorDataset(X_train_tensor, y_train_binary_tensor), batch_size=batch_size, shuffle=True)\n",
    "test_loader_binary = DataLoader(TensorDataset(X_test_tensor, y_test_binary_tensor), batch_size=batch_size)\n",
    "train_loader_ternary = DataLoader(TensorDataset(X_train_tensor, y_train_ternary_tensor), batch_size=batch_size, shuffle=True)\n",
    "test_loader_ternary = DataLoader(TensorDataset(X_test_tensor, y_test_ternary_tensor), batch_size=batch_size)\n",
    "\n",
    "# Initializing models, loss function, and optimizers\n",
    "binary_model = MLP(input_size=300, output_size=2)\n",
    "ternary_model = MLP(input_size=300, output_size=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_binary = optim.Adam(binary_model.parameters())\n",
    "optimizer_ternary = optim.Adam(ternary_model.parameters())\n",
    "\n",
    "# Train binary model and evaluate\n",
    "num_epochs = 10  # You can adjust this\n",
    "train_model(binary_model, optimizer_binary, criterion, train_loader_binary, num_epochs)\n",
    "binary_accuracy = evaluate_model(binary_model, test_loader_binary)\n",
    "print(f'Binary classification accuracy: {binary_accuracy:.2f}')\n",
    "\n",
    "# ternary model\n",
    "train_model(ternary_model, optimizer_ternary, criterion, train_loader_ternary, num_epochs)\n",
    "ternary_accuracy = evaluate_model(ternary_model, test_loader_ternary)\n",
    "print(f'Ternary classification accuracy: {ternary_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "654dc17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique y_train labels: [1 3 2 4 0]\n",
      "Unique y_test labels: [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print('Unique y_train labels:', y_train.unique())\n",
    "print('Unique y_test labels:', y_test.unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2610f0f8",
   "metadata": {},
   "source": [
    "# 5. CONVOLUTIONAL NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0749ce6",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 22.4 GiB for an array with shape (200000, 50, 300) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Assuming X_train, X_test, and my_model are already defined\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m X_train_cnn \u001b[38;5;241m=\u001b[39m prepare_cnn_data(X_train, my_model, sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[0;32m     19\u001b[0m X_test_cnn \u001b[38;5;241m=\u001b[39m prepare_cnn_data(X_test, my_model, sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n",
      "Cell \u001b[1;32mIn[40], line 6\u001b[0m, in \u001b[0;36mprepare_cnn_data\u001b[1;34m(reviews, model, sequence_length, vector_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_cnn_data\u001b[39m(reviews, model, sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(reviews), sequence_length, vector_size))\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, review \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(reviews):\n\u001b[0;32m      9\u001b[0m         words \u001b[38;5;241m=\u001b[39m review\u001b[38;5;241m.\u001b[39msplit()[:sequence_length]\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 22.4 GiB for an array with shape (200000, 50, 300) and data type float64"
     ]
    }
   ],
   "source": [
    "#5. Convolutional Neural Networks \n",
    "\n",
    "# Preparing the data\n",
    "\n",
    "def prepare_cnn_data(reviews, model, sequence_length=50, vector_size=300):\n",
    "    data = np.zeros((len(reviews), sequence_length, vector_size))\n",
    "    \n",
    "    for i, review in enumerate(reviews):\n",
    "        words = review.split()[:sequence_length]\n",
    "        for j, word in enumerate(words):\n",
    "            if word in model.wv.key_to_index:\n",
    "                data[i, j, :] = model.wv[word]\n",
    "    \n",
    "       return data\n",
    "\n",
    "X_train_cnn = prepare_cnn_data(X_train, my_model, sequence_length=50, vector_size=300)\n",
    "X_test_cnn = prepare_cnn_data(X_test, my_model, sequence_length=50, vector_size=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168082fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Simple CNN Model\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_channels, sequence_length, output_size):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 50, kernel_size=5, padding=2)  # Output: 50 channels\n",
    "        self.conv2 = nn.Conv1d(50, 10, kernel_size=5, padding=2)  # Output: 10 channels\n",
    "        self.fc = nn.Linear(10 * sequence_length, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluating the CNN\n",
    "\n",
    "# Convert the embeddings and labels into PyTorch tensors for CNN\n",
    "X_train_tensor_cnn = torch.tensor(X_train_cnn, dtype=torch.float32)\n",
    "X_test_tensor_cnn = torch.tensor(X_test_cnn, dtype=torch.float32)\n",
    "\n",
    "train_data_cnn = TensorDataset(X_train_tensor_cnn, y_train_tensor)\n",
    "test_data_cnn = TensorDataset(X_test_tensor_cnn, y_test_tensor)\n",
    "\n",
    "train_loader_cnn = DataLoader(train_data_cnn, batch_size=64, shuffle=True)\n",
    "test_loader_cnn = DataLoader(test_data_cnn, batch_size=64)\n",
    "\n",
    "# Initialize the CNN\n",
    "cnn_model = SimpleCNN(input_channels=300, sequence_length=50, output_size=2)  # Use output_size=3 for ternary classification\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters())\n",
    "\n",
    "# Train \n",
    "train_model(cnn_model, optimizer, criterion, train_loader_cnn, num_epochs=10)  # Adjust num_epochs as needed\n",
    "\n",
    "# Evaluate \n",
    "accuracy = evaluate_model(cnn_model, test_loader_cnn)\n",
    "print(f'CNN classification accuracy: {accuracy:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
