{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf44d7c",
   "metadata": {},
   "source": [
    "# TASK 1: VOCABULARY CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "061cbafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 16920\n",
      "Total occurrences of '<unk>': 32537\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "# Reading the Training Data\n",
    "file_path = r\"C:\\Users\\adity\\Desktop\\NLP\\CSCI544_HW3\\hw3\\data\\train\"  \n",
    "word_counts = defaultdict(int)\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if line.strip():  # skips blank lines\n",
    "            _, word, _ = line.strip().split('\\t')\n",
    "            word_counts[word] += 1\n",
    "\n",
    "#  Counting Word Occurrences and Replacing Rare Words\n",
    "threshold = 3\n",
    "vocabulary = {'<unk>': 0}\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold:\n",
    "        vocabulary[word] = count\n",
    "    else:\n",
    "        vocabulary['<unk>'] += count\n",
    "\n",
    "#  Sorting and Indexing Vocabulary\n",
    "sorted_vocab = sorted(vocabulary.items(), key=operator.itemgetter(1), reverse=True)\n",
    "indexed_vocab = {word: (index, count) for index, (word, count) in enumerate(sorted_vocab)}\n",
    "\n",
    "#  Write to Vocabulary File\n",
    "with open('vocab.txt', 'w') as vocab_file:\n",
    "    for word, (index, count) in indexed_vocab.items():\n",
    "        vocab_file.write(f\"{index}\\t{word}\\t{count}\\n\")\n",
    "\n",
    "# Display size of vocabulary and total occurrences of '<unk>'\n",
    "print(f\"Total vocabulary size: {len(indexed_vocab)}\")\n",
    "print(f\"Total occurrences of '<unk>': {indexed_vocab['<unk>'][1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "460890c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t,\t46476\n",
      "1\tthe\t39533\n",
      "2\t.\t37452\n",
      "3\t<unk>\t32537\n",
      "4\tof\t22104\n",
      "5\tto\t21305\n",
      "6\ta\t18469\n",
      "7\tand\t15346\n",
      "8\tin\t14609\n",
      "9\t's\t8872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing first 10 lines of the vocab.txt file to check result\n",
    "with open('vocab.txt', 'r') as vocab_file:\n",
    "    lines = [next(vocab_file) for _ in range(10)]\n",
    "    print(''.join(lines))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce340c0",
   "metadata": {},
   "source": [
    "# TASK 2: MODEL LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d72e5620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transition parameters: 1351\n",
      "Number of emission parameters: 50286\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "file_path = r\"C:\\\\Users\\\\adity\\\\Desktop\\\\NLP\\\\CSCI544_HW3\\\\hw3\\\\data\\\\train\"\n",
    "\n",
    "# Initializing dictionaries\n",
    "transition_counts = defaultdict(Counter)\n",
    "emission_counts = defaultdict(Counter)\n",
    "tag_counts = Counter()\n",
    "\n",
    "# Reading training data\n",
    "with open(file_path, 'r') as file:\n",
    "    prev_tag = None  # To keep track of previous tag for transition counts\n",
    "    for line in file:\n",
    "        if line.strip():  \n",
    "            _, word, tag = line.strip().split('\\t')\n",
    "            emission_counts[tag][word] += 1\n",
    "            tag_counts[tag] += 1\n",
    "            if prev_tag is not None:  # If not start of the file\n",
    "                transition_counts[prev_tag][tag] += 1\n",
    "            prev_tag = tag\n",
    "        else:  # Reset prev_tag at end\n",
    "            prev_tag = None\n",
    "\n",
    "# Calculating transition probabilities\n",
    "transition_probs = {s: {s_prime: count / tag_counts[s] for s_prime, count in s_counts.items()}\n",
    "                    for s, s_counts in transition_counts.items()}\n",
    "\n",
    "# Calculating emission probabilities\n",
    "emission_probs = {tag: {word: count / tag_counts[tag] for word, count in word_counts.items()}\n",
    "                  for tag, word_counts in emission_counts.items()}\n",
    "\n",
    "# Write probabilities to JSON file\n",
    "hmm_model = {'transition': transition_probs, 'emission': emission_probs}\n",
    "with open('hmm.json', 'w') as f:\n",
    "    json.dump(hmm_model, f, indent=2)\n",
    "\n",
    "# Output no. of transition and emission parameters\n",
    "print(f\"Number of transition parameters: {sum(len(s_counts) for s_counts in transition_probs.values())}\")\n",
    "print(f\"Number of emission parameters: {sum(len(word_counts) for word_counts in emission_probs.values())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c76f693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample transition probabilities:\n",
      "NNP: {'NNP': 0.3782645420509543, ',': 0.13846908958086018, 'CD': 0.019176330928682313, 'VBZ': 0.0391973335768423, 'VBG': 0.0017692448178248561}\n",
      ",: {'CD': 0.021234939759036144, 'MD': 0.010542168674698794, 'DT': 0.1336273666092943, 'VBD': 0.05154905335628227, 'NNS': 0.02732358003442341}\n",
      "CD: {'NNS': 0.15775891730703062, '.': 0.0725427227893107, 'CC': 0.017175134763160915, 'TO': 0.037590319990824635, ',': 0.09548113315747218}\n",
      "NNS: {'JJ': 0.017196978862406887, 'VBZ': 0.008520714149916175, 'IN': 0.2345183981748734, 'VBN': 0.020930192364195715, 'VBD': 0.07125944105497849}\n",
      "JJ: {',': 0.029129343105320303, 'NN': 0.4491042345276873, 'CC': 0.01701615092290988, 'JJ': 0.07400244299674268, 'IN': 0.05652823018458197}\n",
      "\n",
      "Sample emission probabilities:\n",
      "NNP: {'Pierre': 6.84868961738654e-05, 'Vinken': 2.2828965391288468e-05, 'Nov.': 0.0026709889507807506, 'Mr.': 0.044014245274404167, 'Elsevier': 1.1414482695644234e-05}\n",
      ",: {',': 0.9999139414802065, 'Wa': 2.151462994836489e-05, 'an': 2.151462994836489e-05, '2': 2.151462994836489e-05, 'underwriters': 2.151462994836489e-05}\n",
      "CD: {'61': 0.0007168253240050465, '29': 0.0021218029590549374, '55': 0.0015483426998509004, '30': 0.013562335130175478, '1956': 8.601903888060558e-05}\n",
      "NNS: {'years': 0.019530237301024905, 'filters': 0.00015555056257453463, 'deaths': 0.0005012184794068339, 'workers': 0.003404828980798147, 'researchers': 0.0011579875213882024}\n",
      "JJ: {'old': 0.003613599348534202, 'nonexecutive': 0.00010179153094462541, 'former': 0.004377035830618893, 'British': 0.0032742942453854508, 'industrial': 0.002120656894679696}\n"
     ]
    }
   ],
   "source": [
    "# Printing small sample of transition probabilities\n",
    "print(\"Sample transition probabilities:\")\n",
    "for tag, following_tags in list(transition_probs.items())[:5]:\n",
    "    print(f\"{tag}: {dict(list(following_tags.items())[:5])}\")\n",
    "\n",
    "# Printing small sample of emission probabilities\n",
    "print(\"\\nSample emission probabilities:\")\n",
    "for tag, words in list(emission_probs.items())[:5]:\n",
    "    print(f\"{tag}: {dict(list(words.items())[:5])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db8b686",
   "metadata": {},
   "source": [
    "# TASK 3: GREEDY DECODING WITH HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "12d7f512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Loading HMM model\n",
    "with open('hmm.json', 'r') as f:\n",
    "    hmm_model = json.load(f)\n",
    "transition_probs = hmm_model['transition']\n",
    "emission_probs = hmm_model['emission']\n",
    "\n",
    "# Function to predict the tag using greedy decoding\n",
    "def predict_tag(word, prev_tag, transition_probs, emission_probs):\n",
    "    # Initialize max probability and best tag variables\n",
    "    max_prob = 0\n",
    "    best_tag = None\n",
    "    for tag in emission_probs:\n",
    "        # Calculate the emission probability\n",
    "        emission_prob = emission_probs[tag].get(word, 0)\n",
    "        # Calculate the transition probability\n",
    "        transition_prob = transition_probs.get(prev_tag, {}).get(tag, 0)\n",
    "        # Calculate the combined probability\n",
    "        prob = emission_prob * transition_prob\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            best_tag = tag\n",
    "    return best_tag if best_tag else 'NN'  # Default to 'NN' if no tag found\n",
    "\n",
    "# Read development data and predict tags\n",
    "output_lines = []\n",
    "with open(r\"C:\\Users\\adity\\Desktop\\NLP\\CSCI544_HW3\\hw3\\data\\dev\", 'r') as file:\n",
    "    prev_tag = '<s>'  # Start of sentence tag\n",
    "    for line in file:\n",
    "        if line.strip():  \n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 3:\n",
    "                index, word, _ = parts  # Ignore the actual tag\n",
    "            else:\n",
    "                raise ValueError(f\"Line does not contain three tab-separated values: {line}\")\n",
    "            # Predict the tag\n",
    "            predicted_tag = predict_tag(word, prev_tag, transition_probs, emission_probs)\n",
    "            output_lines.append(f\"{index}\\t{word}\\t{predicted_tag}\\n\")\n",
    "            prev_tag = predicted_tag\n",
    "        else: \n",
    "            output_lines.append(\"\\n\")\n",
    "            prev_tag = '<s>'\n",
    "\n",
    "\n",
    "# Writng predictions to a file\n",
    "with open('greedy.out', 'w') as out_file:\n",
    "    out_file.writelines(output_lines)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9b611e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tThe\tNN\n",
      "2\tArizona\tNNP\n",
      "3\tCorporations\tNNS\n",
      "4\tCommission\tNNP\n",
      "5\tauthorized\tVBD\n",
      "6\tan\tDT\n",
      "7\t11.5\tCD\n",
      "8\t%\tNN\n",
      "9\trate\tNN\n",
      "10\tincrease\tNN\n"
     ]
    }
   ],
   "source": [
    "# Printing first 10 lines of the greedy.out file to check the result\n",
    "with open('greedy.out', 'r') as f:\n",
    "    for _ in range(10):\n",
    "        print(f.readline(), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5f9ca675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 131768, correct: 117669, accuracy: 89.30%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating accuracy\n",
    "!python \"C:\\Users\\adity\\Desktop\\NLP\\CSCI544_HW3\\hw3\\eval.py\" -p \"C:\\Users\\adity\\greedy.out\" -g \"C:\\Users\\adity\\Desktop\\NLP\\CSCI544_HW3\\hw3\\data\\dev\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d695f5",
   "metadata": {},
   "source": [
    "# TASK 4: VITERBI DECODING WITH HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7e89b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load HMM model\n",
    "with open('hmm.json', 'r') as f:\n",
    "    hmm_model = json.load(f)\n",
    "transition_probs = hmm_model['transition']\n",
    "emission_probs = hmm_model['emission']\n",
    "states = list(emission_probs.keys())  # Assuming all states emit at least one word\n",
    "\n",
    "#  Viterbi algorithm\n",
    "def viterbi(obs, states, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "\n",
    "    # Initializing start probabilities with default if '<s>' not found\n",
    "    start_p = {state: 1/len(states) for state in states} if '<s>' not in trans_p else trans_p['<s>']\n",
    "\n",
    "    # Base case: Initialize probabilities for first observation\n",
    "    for st in states:\n",
    "        V[0][st] = start_p.get(st, 0) * emit_p[st].get(obs[0], 0)\n",
    "        path[st] = [st]\n",
    "\n",
    "    # Run Viterbi for t > 0\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        newpath = {}\n",
    "\n",
    "        for cur_state in states:\n",
    "            # small probability for unseen transitions/emissions\n",
    "            max_prob = max((V[t-1][prev_state] * trans_p[prev_state].get(cur_state, 1e-6) * emit_p[cur_state].get(obs[t], 1e-6), prev_state) for prev_state in states)\n",
    "            V[t][cur_state], state = max_prob\n",
    "            newpath[cur_state] = path[state] + [cur_state]\n",
    "\n",
    "        path = newpath\n",
    "\n",
    "    # Choosing ending state with highest probability\n",
    "    n = len(obs) - 1\n",
    "    # check if last column in V is empty for ValueError\n",
    "    if all(V[n][state] == 0 for state in states):\n",
    "        # If all probabilities are zero, we can't find a max\n",
    "        \n",
    "        prob, state = 0, states[0] \n",
    "    else:\n",
    "        prob, state = max((V[n][state], state) for state in states)\n",
    "\n",
    "    return (prob, path[state])\n",
    "\n",
    "\n",
    "# Read dev data, apply Viterbi algorithm, and writing predictions\n",
    "output_lines = []\n",
    "dev_file_path = r\"C:\\Users\\adity\\Desktop\\NLP\\CSCI544_HW3\\hw3\\data\\dev\"  \n",
    "with open(dev_file_path, 'r') as file:\n",
    "    sentence = []\n",
    "    for line in file:\n",
    "        if line.strip():\n",
    "            index, word, _ = line.strip().split('\\t')\n",
    "            sentence.append(word)\n",
    "        else:\n",
    "            if sentence:\n",
    "                _, tags = viterbi(sentence, states, transition_probs, emission_probs)\n",
    "                output_lines.extend([f\"{i+1}\\t{word}\\t{tag}\\n\" for i, (word, tag) in enumerate(zip(sentence, tags))])\n",
    "            output_lines.append(\"\\n\")\n",
    "            sentence = []\n",
    "\n",
    "# Writing predictions to the viterbi.out file\n",
    "with open('viterbi.out', 'w') as out_file:\n",
    "    out_file.writelines(output_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "acda172c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tThe\tDT\n",
      "2\tArizona\tNNP\n",
      "3\tCorporations\tNNP\n",
      "4\tCommission\tNNP\n",
      "5\tauthorized\tVBD\n",
      "6\tan\tDT\n",
      "7\t11.5\tCD\n",
      "8\t%\tNN\n",
      "9\trate\tNN\n",
      "10\tincrease\tNN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print first few lines of the viterbi.out file to verify the output\n",
    "with open('viterbi.out', 'r') as f:\n",
    "    lines = [next(f) for _ in range(10)]\n",
    "    print(''.join(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ba14e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1\\tThat\\tDT' '38\\t.\\t.' 131751\n",
      "total: 131751, correct: 116916, accuracy: 88.74%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating accuracy\n",
    "!python \"C:\\Users\\adity\\Desktop\\NLP\\CSCI544_HW3\\hw3\\eval.py\" -p \"C:\\Users\\adity\\viterbi.out\" -g \"C:\\Users\\adity\\Desktop\\NLP\\CSCI544_HW3\\hw3\\data\\dev\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c169bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
